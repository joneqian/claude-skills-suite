---
name: langgraph
description: LangGraph Python framework for building stateful, multi-actor LLM applications. Use for agent workflows, persistence, durable execution, streaming, interrupts, time travel, memory, subgraphs, and production deployment.
---

# Langgraph Skill

Langgraph python framework for building stateful, multi-actor llm applications. use for agent workflows, persistence, durable execution, streaming, interrupts, time travel, memory, subgraphs, and production deployment., generated from official documentation.

## When to Use This Skill

This skill should be triggered when:
- Working with langgraph
- Asking about langgraph features or APIs
- Implementing langgraph solutions
- Debugging langgraph code
- Learning langgraph best practices

## Quick Reference

### Common Patterns

**Pattern 1:** Docs by LangChain home pageLangChain + LangGraphSearch...⌘KSupportGitHubTry LangSmithTry LangSmithSearch...NavigationLangGraph overviewLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartLocal serverChangelogThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureTestLangSmith StudioAgent Chat UILangSmith DeploymentLangSmith ObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this page installCore benefitsLangGraph ecosystemAcknowledgementsLangGraph overviewCopy pageGain control with LangGraph to design agents that reliably handle complex tasksCopy pageTrusted by companies shaping the future of agents— including Klarna, Replit, Elastic, and more— LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents. LangGraph is very low-level, and focused entirely on agent orchestration. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools. We will commonly use LangChain components throughout the documentation to integrate models and tools, but you don’t need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain’s agents that provide pre-built architectures for common LLM and tool-calling loops. LangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more. ​ install pipuvCopypip install -U langgraph Then, create a simple hello world example: Copyfrom langgraph.graph import StateGraph, MessagesState, START, END def mock_llm(state: MessagesState): return {"messages": [{"role": "ai", "content": "hello world"}]} graph = StateGraph(MessagesState) graph.add_node(mock_llm) graph.add_edge(START, "mock_llm") graph.add_edge("mock_llm", END) graph = graph.compile() graph.invoke({"messages": [{"role": "user", "content": "hi!"}]}) ​Core benefits LangGraph provides low-level supporting infrastructure for any long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits: Durable execution: Build agents that persist through failures and can run for extended periods, resuming from where they left off. Human-in-the-loop: Incorporate human oversight by inspecting and modifying agent state at any point. Comprehensive memory: Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions. Debugging with LangSmith: Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics. Production-ready deployment: Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows. ​LangGraph ecosystem While LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with: LangSmithTrace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.Learn moreLangSmith Agent ServerDeploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams — and iterate quickly with visual prototyping in Studio.Learn moreLangChainProvides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.Learn more ​Acknowledgements LangGraph is inspired by Pregel and Apache Beam. The public interface draws inspiration from NetworkX. LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain. Edit this page on GitHub or file an issue. Connect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoInstall LangGraphNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by

```
pip install -U langgraph
```

**Pattern 2:** Docs by LangChain home pageLangChain + LangGraphSearch...⌘KSupportGitHubTry LangSmithTry LangSmithSearch...NavigationCapabilitiesStreamingLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartLocal serverChangelogThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureTestLangSmith StudioAgent Chat UILangSmith DeploymentLangSmith ObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this pageSupported stream modesBasic usage exampleStream multiple modesStream graph stateStream subgraph outputsDebuggingLLM tokensFilter by LLM invocationFilter by nodeStream custom dataUse with any LLMDisable streaming for specific chat modelsAsync with Python < 3.11CapabilitiesStreamingCopy pageCopy pageLangGraph implements a streaming system to surface real-time updates. Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs. What’s possible with LangGraph streaming: Stream graph state — get state updates / values with updates and values modes. Stream subgraph outputs — include outputs from both the parent graph and any nested subgraphs. Stream LLM tokens — capture token streams from anywhere: inside nodes, subgraphs, or tools. Stream custom data — send custom updates or progress signals directly from tool functions. Use multiple streaming modes — choose from values (full state), updates (state deltas), messages (LLM tokens + metadata), custom (arbitrary user data), or debug (detailed traces). ​Supported stream modes Pass one or more of the following stream modes as a list to the stream or astream methods: ModeDescriptionvaluesStreams the full value of the state after each step of the graph.updatesStreams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately.customStreams custom data from inside your graph nodes.messagesStreams 2-tuples (LLM token, metadata) from any graph nodes where an LLM is invoked.debugStreams as much information as possible throughout the execution of the graph. ​Basic usage example LangGraph graphs expose the stream (sync) and astream (async) methods to yield streamed outputs as iterators. Copyfor chunk in graph.stream(inputs, stream_mode="updates"): print(chunk) Extended example: streaming updatesCopyfrom typing import TypedDict from langgraph.graph import StateGraph, START, END class State(TypedDict): topic: str joke: str def refine_topic(state: State): return {"topic": state["topic"] + " and cats"} def generate_joke(state: State): return {"joke": f"This is a joke about {state['topic']}"} graph = ( StateGraph(State) .add_node(refine_topic) .add_node(generate_joke) .add_edge(START, "refine_topic") .add_edge("refine_topic", "generate_joke") .add_edge("generate_joke", END) .compile() ) # The stream() method returns an iterator that yields streamed outputs for chunk in graph.stream( {"topic": "ice cream"}, # Set stream_mode="updates" to stream only the updates to the graph state after each node # Other stream modes are also available. See supported stream modes for details stream_mode="updates", ): print(chunk) Copy{'refineTopic': {'topic': 'ice cream and cats'}} {'generateJoke': {'joke': 'This is a joke about ice cream and cats'}} ​Stream multiple modes You can pass a list as the stream_mode parameter to stream multiple modes at once. The streamed outputs will be tuples of (mode, chunk) where mode is the name of the stream mode and chunk is the data streamed by that mode. Copyfor mode, chunk in graph.stream(inputs, stream_mode=["updates", "custom"]): print(chunk) ​Stream graph state Use the stream modes updates and values to stream the state of the graph as it executes. updates streams the updates to the state after each step of the graph. values streams the full value of the state after each step of the graph. Copyfrom typing import TypedDict from langgraph.graph import StateGraph, START, END class State(TypedDict): topic: str joke: str def refine_topic(state: State): return {"topic": state["topic"] + " and cats"} def generate_joke(state: State): return {"joke": f"This is a joke about {state['topic']}"} graph = ( StateGraph(State) .add_node(refine_topic) .add_node(generate_joke) .add_edge(START, "refine_topic") .add_edge("refine_topic", "generate_joke") .add_edge("generate_joke", END) .compile() ) updates valuesUse this to stream only the state updates returned by the nodes after each step. The streamed outputs include the name of the node as well as the update.Copyfor chunk in graph.stream( {"topic": "ice cream"}, stream_mode="updates", ): print(chunk) Use this to stream the full state of the graph after each step.Copyfor chunk in graph.stream( {"topic": "ice cream"}, stream_mode="values", ): print(chunk) ​Stream subgraph outputs To include outputs from subgraphs in the streamed outputs, you can set subgraphs=True in the .stream() method of the parent graph. This will stream outputs from both the parent graph and any subgraphs. The outputs will be streamed as tuples (namespace, data), where namespace is a tuple with the path to the node where a subgraph is invoked, e.g. ("parent_node:<task_id>", "child_node:<task_id>"). Copyfor chunk in graph.stream( {"foo": "foo"}, # Set subgraphs=True to stream outputs from subgraphs subgraphs=True, stream_mode="updates", ): print(chunk) Extended example: streaming from subgraphsCopyfrom langgraph.graph import START, StateGraph from typing import TypedDict # Define subgraph class SubgraphState(TypedDict): foo: str # note that this key is shared with the parent graph state bar: str def subgraph_node_1(state: SubgraphState): return {"bar": "bar"} def subgraph_node_2(state: SubgraphState): return {"foo": state["foo"] + state["bar"]} subgraph_builder = StateGraph(SubgraphState) subgraph_builder.add_node(subgraph_node_1) subgraph_builder.add_node(subgraph_node_2) subgraph_builder.add_edge(START, "subgraph_node_1") subgraph_builder.add_edge("subgraph_node_1", "subgraph_node_2") subgraph = subgraph_builder.compile() # Define parent graph class ParentState(TypedDict): foo: str def node_1(state: ParentState): return {"foo": "hi! " + state["foo"]} builder = StateGraph(ParentState) builder.add_node("node_1", node_1) builder.add_node("node_2", subgraph) builder.add_edge(START, "node_1") builder.add_edge("node_1", "node_2") graph = builder.compile() for chunk in graph.stream( {"foo": "foo"}, stream_mode="updates", # Set subgraphs=True to stream outputs from subgraphs subgraphs=True, ): print(chunk) Copy((), {'node_1': {'foo': 'hi! foo'}}) (('node_2:dfddc4ba-c3c5-6887-5012-a243b5b377c2',), {'subgraph_node_1': {'bar': 'bar'}}) (('node_2:dfddc4ba-c3c5-6887-5012-a243b5b377c2',), {'subgraph_node_2': {'foo': 'hi! foobar'}}) ((), {'node_2': {'foo': 'hi! foobar'}}) Note that we are receiving not just the node updates, but we also the namespaces which tell us what graph (or subgraph) we are streaming from. ​Debugging Use the debug streaming mode to stream as much information as possible throughout the execution of the graph. The streamed outputs include the name of the node as well as the full state. Copyfor chunk in graph.stream( {"topic": "ice cream"}, stream_mode="debug", ): print(chunk) ​LLM tokens Use the messages streaming mode to stream Large Language Model (LLM) outputs token by token from any part of your graph, including nodes, tools, subgraphs, or tasks. The streamed output from messages mode is a tuple (message_chunk, metadata) where: message_chunk: the token or message segment from the LLM. metadata: a dictionary containing details about the graph node and LLM invocation. If your LLM is not available as a LangChain integration, you can stream its outputs using custom mode instead. See use with any LLM for details. Manual config required for async in Python < 3.11 When using Python < 3.11 with async code, you must explicitly pass RunnableConfig to ainvoke() to enable proper streaming. See Async with Python < 3.11 for details or upgrade to Python 3.11+. Copyfrom dataclasses import dataclass from langchain.chat_models import init_chat_model from langgraph.graph import StateGraph, START @dataclass class MyState: topic: str joke: str = "" model = init_chat_model(model="gpt-4o-mini") def call_model(state: MyState): """Call the LLM to generate a joke about a topic""" # Note that message events are emitted even when the LLM is run using .invoke rather than .stream model_response = model.invoke( [ {"role": "user", "content": f"Generate a joke about {state.topic}"} ] ) return {"joke": model_response.content} graph = ( StateGraph(MyState) .add_node(call_model) .add_edge(START, "call_model") .compile() ) # The "messages" stream mode returns an iterator of tuples (message_chunk, metadata) # where message_chunk is the token streamed by the LLM and metadata is a dictionary # with information about the graph node where the LLM was called and other information for message_chunk, metadata in graph.stream( {"topic": "ice cream"}, stream_mode="messages", ): if message_chunk.content: print(message_chunk.content, end="|", flush=True) ​Filter by LLM invocation You can associate tags with LLM invocations to filter the streamed tokens by LLM invocation. Copyfrom langchain.chat_models import init_chat_model # model_1 is tagged with "joke" model_1 = init_chat_model(model="gpt-4o-mini", tags=['joke']) # model_2 is tagged with "poem" model_2 = init_chat_model(model="gpt-4o-mini", tags=['poem']) graph = ... # define a graph that uses these LLMs # The stream_mode is set to "messages" to stream LLM tokens # The metadata contains information about the LLM invocation, including the tags async for msg, metadata in graph.astream( {"topic": "cats"}, stream_mode="messages", ): # Filter the streamed tokens by the tags field in the metadata to only include # the tokens from the LLM invocation with the "joke" tag if metadata["tags"] == ["joke"]: print(msg.content, end="|", flush=True) Extended example: filtering by tagsCopyfrom typing import TypedDict from langchain.chat_models import init_chat_model from langgraph.graph import START, StateGraph # The joke_model is tagged with "joke" joke_model = init_chat_model(model="gpt-4o-mini", tags=["joke"]) # The poem_model is tagged with "poem" poem_model = init_chat_model(model="gpt-4o-mini", tags=["poem"]) class State(TypedDict): topic: str joke: str poem: str async def call_model(state, config): topic = state["topic"] print("Writing joke...") # Note: Passing the config through explicitly is required for python < 3.11 # Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks # The config is passed through explicitly to ensure the context vars are propagated correctly # This is required for Python < 3.11 when using async code. Please see the async section for more details joke_response = await joke_model.ainvoke( [{"role": "user", "content": f"Write a joke about {topic}"}], config, ) print("\n\nWriting poem...") poem_response = await poem_model.ainvoke( [{"role": "user", "content": f"Write a short poem about {topic}"}], config, ) return {"joke": joke_response.content, "poem": poem_response.content} graph = ( StateGraph(State) .add_node(call_model) .add_edge(START, "call_model") .compile() ) # The stream_mode is set to "messages" to stream LLM tokens # The metadata contains information about the LLM invocation, including the tags async for msg, metadata in graph.astream( {"topic": "cats"}, stream_mode="messages", ): if metadata["tags"] == ["joke"]: print(msg.content, end="|", flush=True) ​Filter by node To stream tokens only from specific nodes, use stream_mode="messages" and filter the outputs by the langgraph_node field in the streamed metadata: Copy# The "messages" stream mode returns a tuple of (message_chunk, metadata) # where message_chunk is the token streamed by the LLM and metadata is a dictionary # with information about the graph node where the LLM was called and other information for msg, metadata in graph.stream( inputs, stream_mode="messages", ): # Filter the streamed tokens by the langgraph_node field in the metadata # to only include the tokens from the specified node if msg.content and metadata["langgraph_node"] == "some_node_name": ... Extended example: streaming LLM tokens from specific nodesCopyfrom typing import TypedDict from langgraph.graph import START, StateGraph from langchain_openai import ChatOpenAI model = ChatOpenAI(model="gpt-4o-mini") class State(TypedDict): topic: str joke: str poem: str def write_joke(state: State): topic = state["topic"] joke_response = model.invoke( [{"role": "user", "content": f"Write a joke about {topic}"}] ) return {"joke": joke_response.content} def write_poem(state: State): topic = state["topic"] poem_response = model.invoke( [{"role": "user", "content": f"Write a short poem about {topic}"}] ) return {"poem": poem_response.content} graph = ( StateGraph(State) .add_node(write_joke) .add_node(write_poem) # write both the joke and the poem concurrently .add_edge(START, "write_joke") .add_edge(START, "write_poem") .compile() ) # The "messages" stream mode returns a tuple of (message_chunk, metadata) # where message_chunk is the token streamed by the LLM and metadata is a dictionary # with information about the graph node where the LLM was called and other information for msg, metadata in graph.stream( {"topic": "cats"}, stream_mode="messages", ): # Filter the streamed tokens by the langgraph_node field in the metadata # to only include the tokens from the write_poem node if msg.content and metadata["langgraph_node"] == "write_poem": print(msg.content, end="|", flush=True) ​Stream custom data To send custom user-defined data from inside a LangGraph node or tool, follow these steps: Use get_stream_writer to access the stream writer and emit custom data. Set stream_mode="custom" when calling .stream() or .astream() to get the custom data in the stream. You can combine multiple modes (e.g., ["updates", "custom"]), but at least one must be "custom". No get_stream_writer in async for Python < 3.11 In async code running on Python < 3.11, get_stream_writer will not work. Instead, add a writer parameter to your node or tool and pass it manually. See Async with Python < 3.11 for usage examples. node toolCopyfrom typing import TypedDict from langgraph.config import get_stream_writer from langgraph.graph import StateGraph, START class State(TypedDict): query: str answer: str def node(state: State): # Get the stream writer to send custom data writer = get_stream_writer() # Emit a custom key-value pair (e.g., progress update) writer({"custom_key": "Generating custom data inside node"}) return {"answer": "some data"} graph = ( StateGraph(State) .add_node(node) .add_edge(START, "node") .compile() ) inputs = {"query": "example"} # Set stream_mode="custom" to receive the custom data in the stream for chunk in graph.stream(inputs, stream_mode="custom"): print(chunk) Copyfrom langchain.tools import tool from langgraph.config import get_stream_writer @tool def query_database(query: str) -> str: """Query the database.""" # Access the stream writer to send custom data writer = get_stream_writer() # Emit a custom key-value pair (e.g., progress update) writer({"data": "Retrieved 0/100 records", "type": "progress"}) # perform query # Emit another custom key-value pair writer({"data": "Retrieved 100/100 records", "type": "progress"}) return "some-answer" graph = ... # define a graph that uses this tool # Set stream_mode="custom" to receive the custom data in the stream for chunk in graph.stream(inputs, stream_mode="custom"): print(chunk) ​Use with any LLM You can use stream_mode="custom" to stream data from any LLM API — even if that API does not implement the LangChain chat model interface. This lets you integrate raw LLM clients or external services that provide their own streaming interfaces, making LangGraph highly flexible for custom setups. Copyfrom langgraph.config import get_stream_writer def call_arbitrary_model(state): """Example node that calls an arbitrary model and streams the output""" # Get the stream writer to send custom data writer = get_stream_writer() # Assume you have a streaming client that yields chunks # Generate LLM tokens using your custom streaming client for chunk in your_custom_streaming_client(state["topic"]): # Use the writer to send custom data to the stream writer({"custom_llm_chunk": chunk}) return {"result": "completed"} graph = ( StateGraph(State) .add_node(call_arbitrary_model) # Add other nodes and edges as needed .compile() ) # Set stream_mode="custom" to receive the custom data in the stream for chunk in graph.stream( {"topic": "cats"}, stream_mode="custom", ): # The chunk will contain the custom data streamed from the llm print(chunk) Extended example: streaming arbitrary chat modelCopyimport operator import json from typing import TypedDict from typing_extensions import Annotated from langgraph.graph import StateGraph, START from openai import AsyncOpenAI openai_client = AsyncOpenAI() model_name = "gpt-4o-mini" async def stream_tokens(model_name: str, messages: list[dict]): response = await openai_client.chat.completions.create( messages=messages, model=model_name, stream=True ) role = None async for chunk in response: delta = chunk.choices[0].delta if delta.role is not None: role = delta.role if delta.content: yield {"role": role, "content": delta.content} # this is our tool async def get_items(place: str) -> str: """Use this tool to list items one might find in a place you're asked about.""" writer = get_stream_writer() response = "" async for msg_chunk in stream_tokens( model_name, [ { "role": "user", "content": ( "Can you tell me what kind of items " f"i might find in the following place: '{place}'. " "List at least 3 such items separating them by a comma. " "And include a brief description of each item." ), } ], ): response += msg_chunk["content"] writer(msg_chunk) return response class State(TypedDict): messages: Annotated[list[dict], operator.add] # this is the tool-calling graph node async def call_tool(state: State): ai_message = state["messages"][-1] tool_call = ai_message["tool_calls"][-1] function_name = tool_call["function"]["name"] if function_name != "get_items": raise ValueError(f"Tool {function_name} not supported") function_arguments = tool_call["function"]["arguments"] arguments = json.loads(function_arguments) function_response = await get_items(**arguments) tool_message = { "tool_call_id": tool_call["id"], "role": "tool", "name": function_name, "content": function_response, } return {"messages": [tool_message]} graph = ( StateGraph(State) .add_node(call_tool) .add_edge(START, "call_tool") .compile() ) Let’s invoke the graph with an AIMessage that includes a tool call:Copyinputs = { "messages": [ { "content": None, "role": "assistant", "tool_calls": [ { "id": "1", "function": { "arguments": '{"place":"bedroom"}', "name": "get_items", }, "type": "function", } ], } ] } async for chunk in graph.astream( inputs, stream_mode="custom", ): print(chunk["content"], end="|", flush=True) ​Disable streaming for specific chat models If your application mixes models that support streaming with those that do not, you may need to explicitly disable streaming for models that do not support it. Set streaming=False when initializing the model. init_chat_model Chat model interfaceCopyfrom langchain.chat_models import init_chat_model model = init_chat_model( "claude-sonnet-4-5-20250929", # Set streaming=False to disable streaming for the chat model streaming=False ) Copyfrom langchain_openai import ChatOpenAI # Set streaming=False to disable streaming for the chat model model = ChatOpenAI(model="o1-preview", streaming=False) Not all chat model integrations support the streaming parameter. If your model doesn’t support it, use disable_streaming=True instead. This parameter is available on all chat models via the base class. ​Async with Python < 3.11 In Python versions < 3.11, asyncio tasks do not support the context parameter. This limits LangGraph ability to automatically propagate context, and affects LangGraph’s streaming mechanisms in two key ways: You must explicitly pass RunnableConfig into async LLM calls (e.g., ainvoke()), as callbacks are not automatically propagated. You cannot use get_stream_writer in async nodes or tools — you must pass a writer argument directly. Extended example: async LLM call with manual configCopyfrom typing import TypedDict from langgraph.graph import START, StateGraph from langchain.chat_models import init_chat_model model = init_chat_model(model="gpt-4o-mini") class State(TypedDict): topic: str joke: str # Accept config as an argument in the async node function async def call_model(state, config): topic = state["topic"] print("Generating joke...") # Pass config to model.ainvoke() to ensure proper context propagation joke_response = await model.ainvoke( [{"role": "user", "content": f"Write a joke about {topic}"}], config, ) return {"joke": joke_response.content} graph = ( StateGraph(State) .add_node(call_model) .add_edge(START, "call_model") .compile() ) # Set stream_mode="messages" to stream LLM tokens async for chunk, metadata in graph.astream( {"topic": "ice cream"}, stream_mode="messages", ): if chunk.content: print(chunk.content, end="|", flush=True) Extended example: async custom streaming with stream writerCopyfrom typing import TypedDict from langgraph.types import StreamWriter class State(TypedDict): topic: str joke: str # Add writer as an argument in the function signature of the async node or tool # LangGraph will automatically pass the stream writer to the function async def generate_joke(state: State, writer: StreamWriter): writer({"custom_key": "Streaming custom data while generating a joke"}) return {"joke": f"This is a joke about {state['topic']}"} graph = ( StateGraph(State) .add_node(generate_joke) .add_edge(START, "generate_joke") .compile() ) # Set stream_mode="custom" to receive the custom data in the stream # async for chunk in graph.astream( {"topic": "ice cream"}, stream_mode="custom", ): print(chunk) Edit this page on GitHub or file an issue. Connect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoDurable executionPreviousInterruptsNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by

```
updates
```

**Pattern 3:** Docs by LangChain home pageLangChain + LangGraphSearch...⌘KSupportGitHubTry LangSmithTry LangSmithSearch...NavigationCapabilitiesPersistenceLangChainLangGraphDeep AgentsIntegrationsLearnReferenceContributePythonOverviewGet startedInstallQuickstartLocal serverChangelogThinking in LangGraphWorkflows + agentsCapabilitiesPersistenceDurable executionStreamingInterruptsTime travelMemorySubgraphsProductionApplication structureTestLangSmith StudioAgent Chat UILangSmith DeploymentLangSmith ObservabilityLangGraph APIsGraph APIFunctional APIRuntimeOn this pageThreadsCheckpointsGet stateGet state historyReplayUpdate stateconfigvaluesas_nodeMemory storeBasic usageSemantic searchUsing in LangGraphCheckpointer librariesCheckpointer interfaceSerializerSerialization with pickleEncryptionCapabilitiesHuman-in-the-loopMemoryTime travelFault-tolerancePending writesCapabilitiesPersistenceCopy pageCopy pageLangGraph has a built-in persistence layer, implemented through checkpointers. When you compile a graph with a checkpointer, the checkpointer saves a checkpoint of the graph state at every super-step. Those checkpoints are saved to a thread, which can be accessed after graph execution. Because threads allow access to graph’s state after execution, several powerful capabilities including human-in-the-loop, memory, time travel, and fault-tolerance are all possible. Below, we’ll discuss each of these concepts in more detail. Agent Server handles checkpointing automatically When using the Agent Server, you don’t need to implement or configure checkpointers manually. The server handles all persistence infrastructure for you behind the scenes. ​Threads A thread is a unique ID or thread identifier assigned to each checkpoint saved by a checkpointer. It contains the accumulated state of a sequence of runs. When a run is executed, the state of the underlying graph of the assistant will be persisted to the thread. When invoking a graph with a checkpointer, you must specify a thread_id as part of the configurable portion of the config: Copy{"configurable": {"thread_id": "1"}} A thread’s current and historical state can be retrieved. To persist state, a thread must be created prior to executing a run. The LangSmith API provides several endpoints for creating and managing threads and thread state. See the API reference for more details. The checkpointer uses thread_id as the primary key for storing and retrieving checkpoints. Without it, the checkpointer cannot save state or resume execution after an interrupt, since the checkpointer uses thread_id to load the saved state. ​Checkpoints The state of a thread at a particular point in time is called a checkpoint. Checkpoint is a snapshot of the graph state saved at each super-step and is represented by StateSnapshot object with the following key properties: config: Config associated with this checkpoint. metadata: Metadata associated with this checkpoint. values: Values of the state channels at this point in time. next A tuple of the node names to execute next in the graph. tasks: A tuple of PregelTask objects that contain information about next tasks to be executed. If the step was previously attempted, it will include error information. If a graph was interrupted dynamically from within a node, tasks will contain additional data associated with interrupts. Checkpoints are persisted and can be used to restore the state of a thread at a later time. Let’s see what checkpoints are saved when a simple graph is invoked as follows: Copyfrom langgraph.graph import StateGraph, START, END from langgraph.checkpoint.memory import InMemorySaver from langchain_core.runnables import RunnableConfig from typing import Annotated from typing_extensions import TypedDict from operator import add class State(TypedDict): foo: str bar: Annotated[list[str], add] def node_a(state: State): return {"foo": "a", "bar": ["a"]} def node_b(state: State): return {"foo": "b", "bar": ["b"]} workflow = StateGraph(State) workflow.add_node(node_a) workflow.add_node(node_b) workflow.add_edge(START, "node_a") workflow.add_edge("node_a", "node_b") workflow.add_edge("node_b", END) checkpointer = InMemorySaver() graph = workflow.compile(checkpointer=checkpointer) config: RunnableConfig = {"configurable": {"thread_id": "1"}} graph.invoke({"foo": "", "bar":[]}, config) After we run the graph, we expect to see exactly 4 checkpoints: Empty checkpoint with START as the next node to be executed Checkpoint with the user input {'foo': '', 'bar': []} and node_a as the next node to be executed Checkpoint with the outputs of node_a {'foo': 'a', 'bar': ['a']} and node_b as the next node to be executed Checkpoint with the outputs of node_b {'foo': 'b', 'bar': ['a', 'b']} and no next nodes to be executed Note that we bar channel values contain outputs from both nodes as we have a reducer for bar channel. ​Get state When interacting with the saved graph state, you must specify a thread identifier. You can view the latest state of the graph by calling graph.get_state(config). This will return a StateSnapshot object that corresponds to the latest checkpoint associated with the thread ID provided in the config or a checkpoint associated with a checkpoint ID for the thread, if provided. Copy# get the latest state snapshot config = {"configurable": {"thread_id": "1"}} graph.get_state(config) # get a state snapshot for a specific checkpoint_id config = {"configurable": {"thread_id": "1", "checkpoint_id": "1ef663ba-28fe-6528-8002-5a559208592c"}} graph.get_state(config) In our example, the output of get_state will look like this: CopyStateSnapshot( values={'foo': 'b', 'bar': ['a', 'b']}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28fe-6528-8002-5a559208592c'}}, metadata={'source': 'loop', 'writes': {'node_b': {'foo': 'b', 'bar': ['b']}}, 'step': 2}, created_at='2024-08-29T19:19:38.821749+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}}, tasks=() ) ​Get state history You can get the full history of the graph execution for a given thread by calling graph.get_state_history(config). This will return a list of StateSnapshot objects associated with the thread ID provided in the config. Importantly, the checkpoints will be ordered chronologically with the most recent checkpoint / StateSnapshot being the first in the list. Copyconfig = {"configurable": {"thread_id": "1"}} list(graph.get_state_history(config)) In our example, the output of get_state_history will look like this: Copy[ StateSnapshot( values={'foo': 'b', 'bar': ['a', 'b']}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28fe-6528-8002-5a559208592c'}}, metadata={'source': 'loop', 'writes': {'node_b': {'foo': 'b', 'bar': ['b']}}, 'step': 2}, created_at='2024-08-29T19:19:38.821749+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}}, tasks=(), ), StateSnapshot( values={'foo': 'a', 'bar': ['a']}, next=('node_b',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}}, metadata={'source': 'loop', 'writes': {'node_a': {'foo': 'a', 'bar': ['a']}}, 'step': 1}, created_at='2024-08-29T19:19:38.819946+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f4-6b4a-8000-ca575a13d36a'}}, tasks=(PregelTask(id='6fb7314f-f114-5413-a1f3-d37dfe98ff44', name='node_b', error=None, interrupts=()),), ), StateSnapshot( values={'foo': '', 'bar': []}, next=('node_a',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f4-6b4a-8000-ca575a13d36a'}}, metadata={'source': 'loop', 'writes': None, 'step': 0}, created_at='2024-08-29T19:19:38.817813+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f0-6c66-bfff-6723431e8481'}}, tasks=(PregelTask(id='f1b14528-5ee5-579c-949b-23ef9bfbed58', name='node_a', error=None, interrupts=()),), ), StateSnapshot( values={'bar': []}, next=('__start__',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f0-6c66-bfff-6723431e8481'}}, metadata={'source': 'input', 'writes': {'foo': ''}, 'step': -1}, created_at='2024-08-29T19:19:38.816205+00:00', parent_config=None, tasks=(PregelTask(id='6d27aa2e-d72b-5504-a36f-8620e54a76dd', name='__start__', error=None, interrupts=()),), ) ] ​Replay It’s also possible to play-back a prior graph execution. If we invoke a graph with a thread_id and a checkpoint_id, then we will re-play the previously executed steps before a checkpoint that corresponds to the checkpoint_id, and only execute the steps after the checkpoint. thread_id is the ID of a thread. checkpoint_id is an identifier that refers to a specific checkpoint within a thread. You must pass these when invoking the graph as part of the configurable portion of the config: Copyconfig = {"configurable": {"thread_id": "1", "checkpoint_id": "0c62ca34-ac19-445d-bbb0-5b4984975b2a"}} graph.invoke(None, config=config) Importantly, LangGraph knows whether a particular step has been executed previously. If it has, LangGraph simply re-plays that particular step in the graph and does not re-execute the step, but only for the steps before the provided checkpoint_id. All of the steps after checkpoint_id will be executed (i.e., a new fork), even if they have been executed previously. See this how to guide on time-travel to learn more about replaying. ​Update state In addition to re-playing the graph from specific checkpoints, we can also edit the graph state. We do this using update_state. This method accepts three different arguments: ​config The config should contain thread_id specifying which thread to update. When only the thread_id is passed, we update (or fork) the current state. Optionally, if we include checkpoint_id field, then we fork that selected checkpoint. ​values These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions, if they are defined for some of the channels in the graph state. This means that update_state does NOT automatically overwrite the channel values for every channel, but only for the channels without reducers. Let’s walk through an example. Let’s assume you have defined the state of your graph with the following schema (see full example above): Copyfrom typing import Annotated from typing_extensions import TypedDict from operator import add class State(TypedDict): foo: int bar: Annotated[list[str], add] Let’s now assume the current state of the graph is Copy{"foo": 1, "bar": ["a"]} If you update the state as below: Copygraph.update_state(config, {"foo": 2, "bar": ["b"]}) Then the new state of the graph will be: Copy{"foo": 2, "bar": ["a", "b"]} The foo key (channel) is completely changed (because there is no reducer specified for that channel, so update_state overwrites it). However, there is a reducer specified for the bar key, and so it appends "b" to the state of bar. ​as_node The final thing you can optionally specify when calling update_state is as_node. If you provided it, the update will be applied as if it came from node as_node. If as_node is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps to execute depend on the last node to have given an update, so this can be used to control which node executes next. See this how to guide on time-travel to learn more about forking state. ​Memory store A state schema specifies a set of keys that are populated as a graph is executed. As discussed above, state can be written by a checkpointer to a thread at each graph step, enabling state persistence. But, what if we want to retain some information across threads? Consider the case of a chatbot where we want to retain specific information about the user across all chat conversations (e.g., threads) with that user! With checkpointers alone, we cannot share information across threads. This motivates the need for the Store interface. As an illustration, we can define an InMemoryStore to store information about a user across threads. We simply compile our graph with a checkpointer, as before, and with our new in_memory_store variable. LangGraph API handles stores automatically When using the LangGraph API, you don’t need to implement or configure stores manually. The API handles all storage infrastructure for you behind the scenes. ​Basic usage First, let’s showcase this in isolation without using LangGraph. Copyfrom langgraph.store.memory import InMemoryStore in_memory_store = InMemoryStore() Memories are namespaced by a tuple, which in this specific example will be (<user_id>, "memories"). The namespace can be any length and represent anything, does not have to be user specific. Copyuser_id = "1" namespace_for_memory = (user_id, "memories") We use the store.put method to save memories to our namespace in the store. When we do this, we specify the namespace, as defined above, and a key-value pair for the memory: the key is simply a unique identifier for the memory (memory_id) and the value (a dictionary) is the memory itself. Copymemory_id = str(uuid.uuid4()) memory = {"food_preference" : "I like pizza"} in_memory_store.put(namespace_for_memory, memory_id, memory) We can read out memories in our namespace using the store.search method, which will return all memories for a given user as a list. The most recent memory is the last in the list. Copymemories = in_memory_store.search(namespace_for_memory) memories[-1].dict() {'value': {'food_preference': 'I like pizza'}, 'key': '07e0caf4-1631-47b7-b15f-65515d4c1843', 'namespace': ['1', 'memories'], 'created_at': '2024-10-02T17:22:31.590602+00:00', 'updated_at': '2024-10-02T17:22:31.590605+00:00'} Each memory type is a Python class (Item) with certain attributes. We can access it as a dictionary by converting via .dict as above. The attributes it has are: value: The value (itself a dictionary) of this memory key: A unique key for this memory in this namespace namespace: A list of strings, the namespace of this memory type created_at: Timestamp for when this memory was created updated_at: Timestamp for when this memory was updated ​Semantic search Beyond simple retrieval, the store also supports semantic search, allowing you to find memories based on meaning rather than exact matches. To enable this, configure the store with an embedding model: Copyfrom langchain.embeddings import init_embeddings store = InMemoryStore( index={ "embed": init_embeddings("openai:text-embedding-3-small"), # Embedding provider "dims": 1536, # Embedding dimensions "fields": ["food_preference", "$"] # Fields to embed } ) Now when searching, you can use natural language queries to find relevant memories: Copy# Find memories about food preferences # (This can be done after putting memories into the store) memories = store.search( namespace_for_memory, query="What does the user like to eat?", limit=3 # Return top 3 matches ) You can control which parts of your memories get embedded by configuring the fields parameter or by specifying the index parameter when storing memories: Copy# Store with specific fields to embed store.put( namespace_for_memory, str(uuid.uuid4()), { "food_preference": "I love Italian cuisine", "context": "Discussing dinner plans" }, index=["food_preference"] # Only embed "food_preferences" field ) # Store without embedding (still retrievable, but not searchable) store.put( namespace_for_memory, str(uuid.uuid4()), {"system_info": "Last updated: 2024-01-01"}, index=False ) ​Using in LangGraph With this all in place, we use the in_memory_store in LangGraph. The in_memory_store works hand-in-hand with the checkpointer: the checkpointer saves state to threads, as discussed above, and the in_memory_store allows us to store arbitrary information for access across threads. We compile the graph with both the checkpointer and the in_memory_store as follows. Copyfrom langgraph.checkpoint.memory import InMemorySaver # We need this because we want to enable threads (conversations) checkpointer = InMemorySaver() # ... Define the graph ... # Compile the graph with the checkpointer and store graph = graph.compile(checkpointer=checkpointer, store=in_memory_store) We invoke the graph with a thread_id, as before, and also with a user_id, which we’ll use to namespace our memories to this particular user as we showed above. Copy# Invoke the graph user_id = "1" config = {"configurable": {"thread_id": "1", "user_id": user_id}} # First let's just say hi to the AI for update in graph.stream( {"messages": [{"role": "user", "content": "hi"}]}, config, stream_mode="updates" ): print(update) We can access the in_memory_store and the user_id in any node by passing store: BaseStore and config: RunnableConfig as node arguments. Here’s how we might use semantic search in a node to find relevant memories: Copydef update_memory(state: MessagesState, config: RunnableConfig, *, store: BaseStore): # Get the user id from the config user_id = config["configurable"]["user_id"] # Namespace the memory namespace = (user_id, "memories") # ... Analyze conversation and create a new memory # Create a new memory ID memory_id = str(uuid.uuid4()) # We create a new memory store.put(namespace, memory_id, {"memory": memory}) As we showed above, we can also access the store in any node and use the store.search method to get memories. Recall the memories are returned as a list of objects that can be converted to a dictionary. Copymemories[-1].dict() {'value': {'food_preference': 'I like pizza'}, 'key': '07e0caf4-1631-47b7-b15f-65515d4c1843', 'namespace': ['1', 'memories'], 'created_at': '2024-10-02T17:22:31.590602+00:00', 'updated_at': '2024-10-02T17:22:31.590605+00:00'} We can access the memories and use them in our model call. Copydef call_model(state: MessagesState, config: RunnableConfig, *, store: BaseStore): # Get the user id from the config user_id = config["configurable"]["user_id"] # Namespace the memory namespace = (user_id, "memories") # Search based on the most recent message memories = store.search( namespace, query=state["messages"][-1].content, limit=3 ) info = "\n".join([d.value["memory"] for d in memories]) # ... Use memories in the model call If we create a new thread, we can still access the same memories so long as the user_id is the same. Copy# Invoke the graph config = {"configurable": {"thread_id": "2", "user_id": "1"}} # Let's say hi again for update in graph.stream( {"messages": [{"role": "user", "content": "hi, tell me about my memories"}]}, config, stream_mode="updates" ): print(update) When we use the LangSmith, either locally (e.g., in Studio) or hosted with LangSmith, the base store is available to use by default and does not need to be specified during graph compilation. To enable semantic search, however, you do need to configure the indexing settings in your langgraph.json file. For example: Copy{ ... "store": { "index": { "embed": "openai:text-embeddings-3-small", "dims": 1536, "fields": ["$"] } } } See the deployment guide for more details and configuration options. ​Checkpointer libraries Under the hood, checkpointing is powered by checkpointer objects that conform to BaseCheckpointSaver interface. LangGraph provides several checkpointer implementations, all implemented via standalone, installable libraries: langgraph-checkpoint: The base interface for checkpointer savers (BaseCheckpointSaver) and serialization/deserialization interface (SerializerProtocol). Includes in-memory checkpointer implementation (InMemorySaver) for experimentation. LangGraph comes with langgraph-checkpoint included. langgraph-checkpoint-sqlite: An implementation of LangGraph checkpointer that uses SQLite database (SqliteSaver / AsyncSqliteSaver). Ideal for experimentation and local workflows. Needs to be installed separately. langgraph-checkpoint-postgres: An advanced checkpointer that uses Postgres database (PostgresSaver / AsyncPostgresSaver), used in LangSmith. Ideal for using in production. Needs to be installed separately. ​Checkpointer interface Each checkpointer conforms to BaseCheckpointSaver interface and implements the following methods: .put - Store a checkpoint with its configuration and metadata. .put_writes - Store intermediate writes linked to a checkpoint (i.e. pending writes). .get_tuple - Fetch a checkpoint tuple using for a given configuration (thread_id and checkpoint_id). This is used to populate StateSnapshot in graph.get_state(). .list - List checkpoints that match a given configuration and filter criteria. This is used to populate state history in graph.get_state_history() If the checkpointer is used with asynchronous graph execution (i.e. executing the graph via .ainvoke, .astream, .abatch), asynchronous versions of the above methods will be used (.aput, .aput_writes, .aget_tuple, .alist). For running your graph asynchronously, you can use InMemorySaver, or async versions of Sqlite/Postgres checkpointers — AsyncSqliteSaver / AsyncPostgresSaver checkpointers. ​Serializer When checkpointers save the graph state, they need to serialize the channel values in the state. This is done using serializer objects. langgraph_checkpoint defines protocol for implementing serializers provides a default implementation (JsonPlusSerializer) that handles a wide variety of types, including LangChain and LangGraph primitives, datetimes, enums and more. ​Serialization with pickle The default serializer, JsonPlusSerializer, uses ormsgpack and JSON under the hood, which is not suitable for all types of objects. If you want to fallback to pickle for objects not currently supported by our msgpack encoder (such as Pandas dataframes), you can use the pickle_fallback argument of the JsonPlusSerializer: Copyfrom langgraph.checkpoint.memory import InMemorySaver from langgraph.checkpoint.serde.jsonplus import JsonPlusSerializer # ... Define the graph ... graph.compile( checkpointer=InMemorySaver(serde=JsonPlusSerializer(pickle_fallback=True)) ) ​Encryption Checkpointers can optionally encrypt all persisted state. To enable this, pass an instance of EncryptedSerializer to the serde argument of any BaseCheckpointSaver implementation. The easiest way to create an encrypted serializer is via from_pycryptodome_aes, which reads the AES key from the LANGGRAPH_AES_KEY environment variable (or accepts a key argument): Copyimport sqlite3 from langgraph.checkpoint.serde.encrypted import EncryptedSerializer from langgraph.checkpoint.sqlite import SqliteSaver serde = EncryptedSerializer.from_pycryptodome_aes() # reads LANGGRAPH_AES_KEY checkpointer = SqliteSaver(sqlite3.connect("checkpoint.db"), serde=serde) Copyfrom langgraph.checkpoint.serde.encrypted import EncryptedSerializer from langgraph.checkpoint.postgres import PostgresSaver serde = EncryptedSerializer.from_pycryptodome_aes() checkpointer = PostgresSaver.from_conn_string("postgresql://...", serde=serde) checkpointer.setup() When running on LangSmith, encryption is automatically enabled whenever LANGGRAPH_AES_KEY is present, so you only need to provide the environment variable. Other encryption schemes can be used by implementing CipherProtocol and supplying it to EncryptedSerializer. ​Capabilities ​Human-in-the-loop First, checkpointers facilitate human-in-the-loop workflows by allowing humans to inspect, interrupt, and approve graph steps. Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. See the how-to guides for examples. ​Memory Second, checkpointers allow for “memory” between interactions. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that thread, which will retain its memory of previous ones. See Add memory for information on how to add and manage conversation memory using checkpointers. ​Time travel Third, checkpointers allow for “time travel”, allowing users to replay prior graph executions to review and / or debug specific graph steps. In addition, checkpointers make it possible to fork the graph state at arbitrary checkpoints to explore alternative trajectories. ​Fault-tolerance Lastly, checkpointing also provides fault-tolerance and error recovery: if one or more nodes fail at a given superstep, you can restart your graph from the last successful step. Additionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don’t re-run the successful nodes. ​Pending writes Additionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don’t re-run the successful nodes. Edit this page on GitHub or file an issue. Connect these docs to Claude, VSCode, and more via MCP for real-time answers.Was this page helpful?YesNoWorkflows and agentsPreviousDurable executionNext⌘IDocs by LangChain home pagegithubxlinkedinyoutubeResourcesForumChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by

```
checkpoint
```

### Example Code Patterns

**Example 1** (markdown):
```markdown
pip install -U langchain
# Requires Python 3.10+
```

**Example 2** (json):
```json
graph.stream(
    {"input": "test"},
    durability="sync"
)
```

**Example 3** (markdown):
```markdown
# Create a new Agent Chat UI project
npx create-agent-chat-app --project-name my-chat-ui
cd my-chat-ui

# Install dependencies and start
pnpm install
pnpm dev
```

**Example 4** (json):
```json
{'b': 'foofoo'}
```

**Example 5** (markdown):
```markdown
# Python >= 3.11 is required.
pip install -U "langgraph-cli[inmem]"
```

## Reference Files

This skill includes comprehensive documentation in `references/`:

- **advanced.md** - Advanced documentation
- **apis.md** - Apis documentation
- **capabilities.md** - Capabilities documentation
- **getting_started.md** - Getting Started documentation
- **production.md** - Production documentation

Use `view` to read specific reference files when detailed information is needed.

## Working with This Skill

### For Beginners
Start with the getting_started or tutorials reference files for foundational concepts.

### For Specific Features
Use the appropriate category reference file (api, guides, etc.) for detailed information.

### For Code Examples
The quick reference section above contains common patterns extracted from the official docs.

## Resources

### references/
Organized documentation extracted from official sources. These files contain:
- Detailed explanations
- Code examples with language annotations
- Links to original documentation
- Table of contents for quick navigation

### scripts/
Add helper scripts here for common automation tasks.

### assets/
Add templates, boilerplate, or example projects here.

## Notes

- This skill was automatically generated from official documentation
- Reference files preserve the structure and examples from source docs
- Code examples include language detection for better syntax highlighting
- Quick reference patterns are extracted from common usage examples in the docs

## Updating

To refresh this skill with updated documentation:
1. Re-run the scraper with the same configuration
2. The skill will be rebuilt with the latest information
